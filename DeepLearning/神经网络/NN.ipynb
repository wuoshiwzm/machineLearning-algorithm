{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 每一个神经元的输出为所有输入的线性组合\n",
    "$$y=f(\\sum\\limits_{i=1}^{n}{{{w}_{i}}{{x}_{i}}-\\theta })$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f(x,W) = Wx +b\n",
    "\n",
    "假设x为1000x1,分为10类，则W为10x1000 ， b为10x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数：\n",
    "$$L=\\frac{1}{N}\\sum\\limits_{i=1}^{N}{\\sum\\limits_{j\\ne {{y}_{i}}}^{N}{mas(0,[f{{({{x}_{i}}:W)}_{j}}-f{{({{x}_{i}};W)}_{{{y}_{i}}}}+1])}}$$\n",
    "\n",
    "正则化的损失函数：\n",
    "$$L=\\frac{1}{N}\\sum\\limits_{i=1}^{N}{\\sum\\limits_{j\\ne {{y}_{i}}}^{N}{mas(0,[f{{({{x}_{i}}:W)}_{j}}-f{{({{x}_{i}};W)}_{{{y}_{i}}}}+1])}}+\\lambda R(W)$$\n",
    "\n",
    "R(W)有各种不同的版本，如$R(W) = W^2$\n",
    "\n",
    "最终的损失函数：\n",
    "$$L=\\frac{1}{N}\\sum\\limits_{i=1}^{N}{\\sum\\limits_{j\\ne {{y}_{i}}}^{N}{mas(0,[f{{({{x}_{i}}:W)}_{j}}-f{{({{x}_{i}};W)}_{{{y}_{i}}}}+1])}}+\\lambda \\sum\\limits_{k}^{{}}{\\sum\\limits_{l}^{{}}{W_{k,l}^{2}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax 分类器\n",
    "\n",
    "在数学，尤其是概率论和相关领域中，Softmax函数，或称归一化指数函数[1]:198，是逻辑函数的一种推广。它能将一个含任意实数的K维向量  ${\\displaystyle \\mathbf {z} } {\\displaystyle \\mathbf {z} }$ “压缩”到另一个K维实向量  ${\\displaystyle \\sigma (\\mathbf {z} )} {\\displaystyle \\sigma (\\mathbf {z} )}$ 中，使得每一个元素的范围都在 ${\\displaystyle (0,1)} {\\displaystyle (0,1)}$之间，并且所有元素的和为1。\n",
    "\n",
    "该函数的形式通常按下面的式子给出：\n",
    "${{L}_{i}}=-\\log (\\frac{{{e}^{{{f}_{{{y}_{i}}}}}}}{\\sum\\limits_{j}^{{}}{{{e}^{{{f}_{j}}}}}})$\n",
    "\n",
    "sigmoid函数：$g(z)=\\frac{1}{1+{{e}^{-z}}}$\n",
    "损失函数：交叉熵损失（cross-entropy loss）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "实数 映射到->概率值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 几种损失函数计算公式\n",
    "\n",
    "Softmax:${{L}_{i}}=-\\log (\\frac{{{e}^{{{f}_{{{y}_{i}}}}}}}{\\sum\\limits_{j}^{{}}{{{e}^{{{f}_{j}}}}}})$\n",
    "\n",
    "SVM:${{L}_{i}}=\\sum\\limits_{j\\ne {{y}_{{}}}}^{{}}{\\max (0,{{s}_{j}}-{{s}_{{{y}_{i}}}}+1)}$\n",
    "\n",
    "Full Loss:$L=\\frac{1}{N}\\sum\\limits_{i=1}^{N}{{{L}_{i}}+R(W)}$  ($R(W)$为正则化项)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化：\n",
    "\n",
    "## 梯度下降："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层前馈神经网络：multi-layer feedforword neural network\n",
    "\n",
    "## BP算法：error BackPropagation 误差逆传播\n",
    "\n",
    "现在最成功的神经网络学习算法，现实任务中使用神经网络时，大多使用BP算法进行训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
