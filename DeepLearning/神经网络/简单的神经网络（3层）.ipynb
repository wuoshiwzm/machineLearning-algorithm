{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数： 前向传播需要，反向传播也需要\n",
    "# deriv 为False表示不求导，为True时表示求导(反向传播)\n",
    "def sigmoid(x,deriv = False):\n",
    "    if(deriv == True):\n",
    "        #这时是反向传播，这里的x代表输出，即sigmod函数，求导就是对sigmod函数求导\n",
    "        return x*(1-x)\n",
    "    else:\n",
    "        #这时直接就是sigmod函数自己\n",
    "        return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5L, 3L)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定输入和对应的label值（类型）\n",
    "# 数据的维度是3维的\n",
    "x = np.array([\n",
    "    [0,0,1],\n",
    "    [0,1,1],\n",
    "    [1,0,1],\n",
    "    [1,1,1],\n",
    "    [0,1,0]    \n",
    "])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5L, 1L)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 给定输入的label值 神经网络是有监督学习\n",
    "y=np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "    [1]\n",
    "])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造神网络的结构  L0->L1->L2  L0输入  L1隐层  L2输出层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 权重参数W的构造与初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.89735203,  0.29798364, -0.97961169,  0.22406371],\n",
       "       [ 0.91849292,  0.28425644, -0.87330526, -0.2862429 ],\n",
       "       [ 0.31384908, -0.70837014,  0.42788558, -0.62640137]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正向传播：x0*w0 -> x1*w1 -> y\n",
    "# 反向传播：\n",
    "# L0层：对应参数W0，输入为3维（列）的，则w0一定是3行，\n",
    "# 又因为输出为L1层，这里假设L1层有4个神经元，则输出为4列的\n",
    "# 则w0为3x4\n",
    "\n",
    "# l2层假设只有一个输出值，对应分类判断[0，1],则同理w1为4x1\n",
    "\n",
    "w0 = np.random.random((3,4))\n",
    "w1 = np.random.random((4,1))\n",
    "# 此时w0, w1为（0，1）区间，希望其为(-1,+1)区间 ：\n",
    "\n",
    "w0 = 2*np.random.random((3,4))-1\n",
    "w1 = 2*np.random.random((4,1))-1\n",
    "\n",
    "w0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正向转播与反向传播\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/transposs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.11916729 -7.24309022  1.9654356   2.31824582]\n",
      " [ 6.98167316  6.51212519  2.02437063 -3.33944326]\n",
      " [ 3.11048549 -3.39557428  0.41046595 -1.29077698]]\n",
      "[[-10.74832159]\n",
      " [ 14.14979129]\n",
      " [  3.82203088]\n",
      " [  4.55112487]]\n"
     ]
    }
   ],
   "source": [
    "for j in xrange(60000):\n",
    "    #------一次正项传播\n",
    "    #L0层\n",
    "    l0=x\n",
    "    #l1为l0乘权重参数再乘激活函数,参数deriv默认为False表示是前向传播\n",
    "    l1 = sigmoid(np.dot(l0,w0),deriv=False)\n",
    "    #同理得l2\n",
    "    l2 = sigmoid(np.dot(l1,w1),deriv=False)\n",
    "    #损失值\n",
    "    error_l2 = y - l2\n",
    "    #每隔10000次打印出损失值\n",
    "    #if(j%10000) == 0:\n",
    "    #print 'error'+str(np.mean(np.abs(error_l2)))\n",
    "        \n",
    "    #------一次反向传播更新w的值 error_l2相当于权重项，损失越大，错的越多，要更新的量 delta_l2 就越大\n",
    "    #这里是对应相乘 因为都是5x1\n",
    "    delta_l2 = error_l2*sigmoid(l2,deriv=True) #print delta_l2.shape  #print sigmoid(l2,deriv=True).shape\n",
    "    \n",
    "    #l1的损失值\n",
    "    #delta_l2是5x1, w1是4x1,需要对w1转置\n",
    "    error_l1 = delta_l2.dot( np.transpose(w1))  #print error_l1.shape\n",
    "    #得l1 的 调整值 delta_l1,即梯度方向:\n",
    "    delta_l1 = error_l1*sigmoid(l1,deriv=True)\n",
    "    \n",
    "    #更新w ，由于之前是  error_l2 = y - l2， 所以这里是+= ， 否则是减等\n",
    "    w1 += l1.T.dot(delta_l2)\n",
    "    w0 += l0.T.dot(delta_l1)\n",
    "    \n",
    "print w0\n",
    "print w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
